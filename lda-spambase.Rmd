---
title: "Linear Discriminant Analysis - Email vs. Spam"
author: "C. H. Chiu"
date: "2018年7月19日"
output: 
  html_document:
    toc: true
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(1234)
```

## Introduction
The dataset 'spambase' derives from a collectionof spam e-mails and non-spam e-mails. Most of the variables indicate whether a particularwordd or character was frequently occurring in the e-mail. A class variable 'class' determines whether the observation is classfied as 'spam' or normal 'email'.

In this document we're applying linear discriminat analysis for this binary classification problem.

## Analysis
### Preparation
We're loading a few packages before we start the analysis:

* data.table, dplyr: dataset manipulations
* ggplot2: graphs
* caret, MASS: linear discriminant analysis and cross-validation
* plotROC: ROC curve
```{r load packages, message=FALSE, warning=FALSE}
library(data.table)
library(dplyr)
library(MASS)
library(caret)
library(ggplot2)
library(plotROC)
```

### Load in the data
We're using fread() function from 'data.table' library to read in our dataset.
```{r set wd, include=FALSE}
setwd('~/projects/multivariate-and-machine-learning/')
```

```{r load in data}
spamtest = fread('spambase.txt')
setDT(spamtest)
dim(spamtest)
```

As we see there are 4601 rows and 63 variables in this dataset.

### Correlations
Collinearity among variables could be harmful for multivariate analysis. We may identify if there are highly correlated or even identical variables in the datasets. In the following section, we're actually able to find 2 sets of identical variables (4 variables in total) and remove them from the datasets.
```{r correlation}
cor = cor(spamtest[, sapply(spamtest, is.numeric), with = F], method = 'pearson')
cor.column = which(cor > 0.99999, arr.ind = T)[which(cor > 0.99999, arr.ind = T)[,1]!=which(cor > 0.99999, arr.ind = T)[,2],]
spamtest[,rownames(cor.column):=NULL]
```

### Split into training and testing sets
We split 70% of the rows as the training the set with the rest of 30% as the testing set.
```{r train and test sets}
split = 0.7
index = sample(nrow(spamtest), round(nrow(spamtest)*split))
spam.train = spamtest[index]
spam.test  = spamtest[-index]
```

### Exploratory analysis
```{r descriptive analysis}
spam.train %>% summary
ggplot(spam.train, aes(x = class, fill=class)) + 
  geom_bar() + 
  stat_count(aes(label=..count..), geom="text", vjust=-1, size=5) +
  labs(title='Frequency of Emails vs. Spams in Training Set', y='Frequency', x='Class') + ylim(c(0, 3200))
```

We check the basic descriptive statistics for every variables and make a plot of the frequency of each class. As mentioned earlier, only one character variable, which is the class variable, exists and the rest are all numeric. We then examine if the label is imbalanced by using the barplot. If the label is very imbalanced, we may have to consider other metrics other than accuracy to correctly evaluate the model. Fortunately though, the distribution doesn't seem so imbalanced.

### Linear discriminant analysis
We use the training function from 'caret' package for our linear discriminant analysis. Alongside the LDA, 10-fold cross validation is also used to improve the model. 
```{r linear discriminant analysis, warning=FALSE}
ctrl = trainControl(method="cv", summaryFunction=twoClassSummary, classProbs=T, savePredictions = T)
lda.fit = train(class ~ ., data = spam.train, method = "lda", trControl = ctrl)
summary(lda.fit)
```

### Prediciton and accuracy
Now that the predictive model is built, we can use the testing set to evaluate the metrics such as confusion matrix, accuracy and ROC curve.
```{r prediciton and confusion matrix}
lda.pred = predict(lda.fit, spam.test)
actual.value = factor(spam.test$class)
conf_matrix = confusionMatrix(lda.pred, actual.value, positive = 'spam')
conf_matrix$table
conf_matrix$overall[1]
```

The accuracy of the model is around 89%. Not the best result but certainly good enough to correctly classify the binary class. However, if we only care if the spam emails can be correctly identified while the non-spam emails are not the priority, metrics such as sensitivity (recall) or area under ROC curve (AUC) are better alternatives than accuracy.

### Sensitivity and Specificity
```{r sensitivity and specificity}
conf_matrix$byClass[1:2]
```

Accuracy is sort of a balance between sensitivity and specificity. In the spam mail case, we generally want to evaluate how good the spam mails can be correctly identified. Whether the non-spam emails can be correctly identified as non-spam is usually not our concern. Sensitivity above shows how good the model can identify the spam mails while specificity is for non-spam emails. 

From above, we can clearly see the model actually classifies non-spam mails better than spam mails. With this regard, 80% senstitivity might not be good enough to identify spam emails. We may try out other classifiers to see if there are better alternatives than the linear discriminant analysis.
