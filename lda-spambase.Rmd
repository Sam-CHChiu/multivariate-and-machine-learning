---
title: "Linear Discriminant Analysis - Email vs. Spam"
author: "C. H. Chiu"
date: "2018年7月19日"
output: 
  html_document:
    toc: true
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(1234)
```

## Introduction
The dataset 'spambase' derives from a collection of spam e-mails and non-spam e-mails. Most of the variables indicate whether a particularwordd or character was frequently occurring in the e-mail. A class variable 'class' determines whether the observation is classfied as 'spam' or normal 'email'.

In this document we're applying linear discriminant analysis for this binary classification problem.

## Analysis
### Preparation
We load a few packages before we start the analysis:

* data.table, dplyr: dataset manipulations
* ggplot2: graphs
* caret, MASS: linear discriminant analysis and cross-validation
* plotROC: ROC curve
```{r load packages, message=FALSE, warning=FALSE}
library(data.table)
library(dplyr)
library(MASS)
library(caret)
library(ggplot2)
library(plotROC)
```

### Load in the data
We use fread() function from 'data.table' library to read in our dataset.
```{r set wd, include=FALSE}
setwd('~/projects/multivariate-and-machine-learning/')
```

```{r load in data}
spamtest = fread('spambase.txt')
setDT(spamtest)
dim(spamtest)
```

As we see there are 4601 rows and 63 variables in this dataset.

### Correlations
Collinearity among variables could be harmful for multivariate analysis. We may identify if there are highly correlated or even identical variables in the datasets. In the following section, we're actually able to find 2 sets of identical variables (4 variables in total) and remove them from the datasets.
```{r correlation}
cor = cor(spamtest[, sapply(spamtest, is.numeric), with = F], method = 'pearson')
cor.column = which(cor > 0.99999, arr.ind = T)[which(cor > 0.99999, arr.ind = T)[,1]!=which(cor > 0.99999, arr.ind = T)[,2],]
spamtest[,rownames(cor.column):=NULL]
```

### Split into training and testing sets
We split 70% of the rows as the training the set with the rest of 30% as the testing set.
```{r train and test sets}
split = 0.7
index = sample(nrow(spamtest), round(nrow(spamtest)*split))
spam.train = spamtest[index]
spam.test  = spamtest[-index]
```

### Exploratory analysis
```{r descriptive analysis}
spam.train %>% sapply(class)
ggplot(spam.train, aes(x = class, fill=class)) + 
  geom_bar() + 
  stat_count(aes(label=..count..), geom="text", vjust=-1, size=5) +
  labs(title='Frequency of Emails vs. Spams in Training Set', y='Frequency', x='Class') + ylim(c(0, 3200))
```

We check the basic descriptive statistics for every variables and make a plot of the frequency of each class. As mentioned earlier, only one character variable, which is the class variable, exists and the rest are all numeric. We then examine if the label is imbalanced by using the barplot. If the label is very imbalanced, we may have to consider other metrics other than accuracy to correctly evaluate the model. Fortunately though, the distribution doesn't seem so imbalanced.

### Linear discriminant analysis
We use the training function from 'caret' package for our linear discriminant analysis. Alongside the LDA, 10-fold cross validation is also used to improve the model. 
```{r linear discriminant analysis, warning=FALSE}
ctrl = trainControl(method="cv", summaryFunction=twoClassSummary, classProbs=T, savePredictions = T)
lda.fit = train(class ~ ., data = spam.train, method = "lda", trControl = ctrl)
summary(lda.fit)
```

### Prediciton and accuracy
Now that the predictive model is built, we can use the testing set to evaluate the metrics such as confusion matrix, accuracy and ROC curve.
```{r prediciton and confusion matrix}
lda.pred = predict(lda.fit, spam.test)
actual.value = factor(spam.test$class)
conf_matrix = confusionMatrix(lda.pred, actual.value, positive = 'spam')
conf_matrix$table
conf_matrix$overall[1]
```

The accuracy of the model is around 89%. Not the best result but certainly good enough to correctly classify the binary class. However, if we only care if the spam emails can be correctly identified while the non-spam emails are not the priority, metrics such as sensitivity (recall) or area under ROC curve (AUC) are better alternatives than accuracy.

### Sensitivity, Specificity and ROC curve
```{r sensitivity and specificity}
conf_matrix$byClass[1:2]
```

Accuracy is sort of a balance between sensitivity and specificity. In the spam mail case, we generally want to evaluate how good the spam mails can be correctly identified. Whether the non-spam emails can be correctly identified as non-spam is usually not our top concern. Sensitivity above shows how good the model can identify the spam mails while specificity is for non-spam emails. From above, we can clearly see the current model actually classifies non-spam mails better than spam mails.

```{r roc curve, warning=F}
roc.plot = ggplot(lda.fit$pred, aes(m = spam, d = factor(obs, levels = c('spam', 'email')) )) + 
  geom_roc(hjust = -0.4, vjust = 1.5) + coord_equal() + 
  xlab('1 - Specificity') + ylab('Sensitivity') + 
  labs(title='ROC Curve') + theme(plot.title = element_text(hjust = 0.5))
roc.plot
calc_auc(roc.plot)[3]
```
If we construct a ROC curve and calculate the area under curve (AUC), 95% is actually not a bad result for AUC. This is due to the nature that ROC curve takes both sensitivity and specificity into consideration. In our case while the sensitivity of the current model is around 80% (4 out of 5 spam mails can be correctly identified), the specificity on the other hand is around 97%, very close to 100%.

## Conclusion
To evaluate the effectiveness of model, we need to determine what goal we want to achieve beforehand. In conclusion, if the user is trying to identify as many spam mails as possible, then it is suggested to try different classifiers since 80% of sensitivity might not be enough. On the other hand, if the user wants to avoid non-spam emails wrongly classifed as spams, 97% of specificity from the current model is actually a very viable choice.


